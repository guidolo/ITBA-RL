{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import gym \n",
    "\n",
    "from PPO_agent import ReinforceAgent\n",
    "from utils import RunningVariance, get_advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_lr = 0.001\n",
    "actor_lr =  0.001\n",
    "LOSS_CLIPPING = 0.01 # Recomendado por el Paper\n",
    "ENTROPY_LOSS = 0.0 #5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancio los agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_time = time()\n",
    "running_variance = RunningVariance()\n",
    "\n",
    "reinforce_agent = ReinforceAgent(ENV='LunarLander-v2', \n",
    "                                 n_experience_episodes=10, \n",
    "                                 EPISODES=1000, \n",
    "                                 epochs=10, \n",
    "                                 LOSS_CLIPPING=LOSS_CLIPPING,\n",
    "                                 ENTROPY_LOSS=ENTROPY_LOSS,\n",
    "                                 lr=actor_lr, \n",
    "                                 algorithm='PPO', \n",
    "                                 gif_to_board=True, \n",
    "                                 batch_size=64, \n",
    "                                 gamma=0.99)\n",
    "\n",
    "\n",
    "critic_model = reinforce_agent.get_critic_model(lr=critic_lr, \n",
    "                                           hidden_layer_neurons=128,\n",
    "                                           input_shape=[reinforce_agent.nS],\n",
    "                                           output_shape=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(8,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entreno V(s) para que no tenga basura ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Corro episodios con policy random\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "\n",
    "# Les saco la ultima observaci√≥n por que no tiene reward\n",
    "observations = []\n",
    "for i in range(reinforce_agent.n_experience_episodes):\n",
    "    observations.append(obs[i][:-1])\n",
    "observations = np.vstack(observations)\n",
    "\n",
    "# Entreno V(s)\n",
    "history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ciclo de entrenamiento del modelo     ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gsidoni/anaconda3/envs/gym/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "correr en linea de comando: tensorboard --logdir logs/\n",
      "Episode: 51\n",
      "Model on episode 52 improved from -inf to -102.44621711879805. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 103\n",
      "Model on episode 104 did not improved -124.05945068471156. Best saved: -102.44621711879805\n",
      "Episode: 155\n",
      "Model on episode 156 did not improved -190.48929802919614. Best saved: -102.44621711879805\n",
      "Episode: 207\n",
      "Model on episode 208 improved from -102.44621711879805 to -69.98005340370891. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 259\n",
      "Model on episode 260 improved from -69.98005340370891 to -44.97927679578034. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 311\n",
      "Model on episode 312 did not improved -45.300654645070686. Best saved: -44.97927679578034\n",
      "Episode: 363\n",
      "Model on episode 364 improved from -44.97927679578034 to -43.053678909076176. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 415\n",
      "Model on episode 416 improved from -43.053678909076176 to -33.48922941191425. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 467\n",
      "Model on episode 468 improved from -33.48922941191425 to -9.50337180644551. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 519\n",
      "Model on episode 520 did not improved -24.82606453439079. Best saved: -9.50337180644551\n",
      "Episode: 571\n",
      "Model on episode 572 improved from -9.50337180644551 to 10.255007932713642. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 623\n",
      "Model on episode 624 did not improved -14.397317227588367. Best saved: 10.255007932713642\n",
      "Episode: 675\n",
      "Model on episode 676 did not improved -3.4165430079579826. Best saved: 10.255007932713642\n",
      "Episode: 727\n",
      "Model on episode 728 did not improved -4.408799179518271. Best saved: 10.255007932713642\n",
      "Episode: 779\n",
      "Model on episode 780 did not improved 5.4975302891911735. Best saved: 10.255007932713642\n",
      "Episode: 831\n",
      "Model on episode 832 improved from 10.255007932713642 to 10.912473468563139. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 883\n",
      "Model on episode 884 did not improved -0.39408319082281157. Best saved: 10.912473468563139\n",
      "Episode: 935\n",
      "Model on episode 936 did not improved -9.359354228361129. Best saved: 10.912473468563139\n",
      "Episode: 987\n",
      "Model on episode 988 improved from 10.912473468563139 to 16.927571550004373. Saved!\n",
      "add_video needs package moviepy\n",
      "Episode: 1009"
     ]
    }
   ],
   "source": [
    "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
    "    \n",
    "    # Corro episodio con policy que se ir√° entrenando\n",
    "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "    \n",
    "    # Pongo todas las acciones de los distintos episodios juntas\n",
    "    actions = np.vstack(actions) \n",
    "    \n",
    "    # Pongo las predicciones juntas y las guardo como las viejas para pasarselas al modelo\n",
    "    # Las nuevas predicciones ser√° la salida de la red neuronal\n",
    "    old_prediction = np.vstack(preds) \n",
    "    \n",
    "    # Calculo advantages y guardo observaciones sin la √∫ltima observaci√≥n\n",
    "    advantage = []\n",
    "    observations = []\n",
    "    for i in range(reinforce_agent.n_experience_episodes):\n",
    "        values = critic_model.predict(obs[i]) \n",
    "        \n",
    "        advantage.append(get_advantages(values, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))\n",
    "        observations.append(obs[i][:-1])\n",
    "        \n",
    "    advantage = np.vstack(advantage)\n",
    "    observations = np.vstack(observations)\n",
    "    \n",
    "    # Calculo de varianza\n",
    "    for ad in advantage:\n",
    "        running_variance.add(ad)\n",
    "\n",
    "    # Normalizaci√≥n de advantage\n",
    "    advantage = (advantage-advantage.mean()) / advantage.std()\n",
    "    \n",
    "    # Entrenamiento de Policy\n",
    "    history_loss = reinforce_agent.model_train.fit([observations, advantage, old_prediction], \n",
    "                                                   actions, \n",
    "                                                   verbose=0, \n",
    "                                                   epochs=reinforce_agent.epochs, \n",
    "                                                   batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "    # Entrenamiento de V(s)\n",
    "    history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), \n",
    "                                      verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "    # Logue de resultados\n",
    "    reinforce_agent.log_data(reinforce_agent.episode, \n",
    "                      history_loss.history['loss'][0], \n",
    "                      np.mean(ep_len), \n",
    "                      reinforce_agent.get_entropy(old_prediction), \n",
    "                      running_variance.get_variance(), \n",
    "                      history_loss.history['actor_loss'][0], \n",
    "                      time() - initial_time, np.mean(ep_returns[-1]), \n",
    "                      history_critic.history['loss'][0])\n",
    "    \n",
    "reinforce_agent.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/LunarLander-v2/PPO/10_10_64_0.99_0.001_1577719517'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
